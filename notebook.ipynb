{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Ensinando Computadores a Verem: Uma Introdu√ß√£o √† Vis√£o Computacional\n",
       "### Demonstra√ß√µes Pr√°ticas usando YOLOv11\n",
       "\n",
       "Este notebook demonstra os conceitos fundamentais de Vis√£o Computacional atrav√©s de implementa√ß√µes pr√°ticas usando o modelo YOLOv11. Exploraremos tr√™s tarefas principais:\n",
       "\n",
       "1. **Classifica√ß√£o de Imagens**: Identificando o conte√∫do principal de uma imagem\n",
       "2. **Detec√ß√£o de Objetos**: Localizando e classificando m√∫ltiplos objetos em uma cena\n",
       "3. **Segmenta√ß√£o Sem√¢ntica**: Determinando o contorno exato de cada objeto\n",
       "\n",
       "Autor: Daniel Amaral  \n",
       "Institui√ß√£o: UFC - PPGETI\n",
       "\n",
       "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FRIZKkunuEwkbljcKHrUOON1iUqjy-zw?usp=sharing)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üõ†Ô∏è Configura√ß√£o do Ambiente\n",
       "\n",
       "Primeiro, vamos instalar e importar as bibliotecas necess√°rias. A principal biblioteca que utilizaremos √© a `ultralytics`, que cont√©m a implementa√ß√£o do YOLOv11."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "%%capture\n",
       "!pip install ultralytics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Importa√ß√£o das bibliotecas necess√°rias\n",
       "import cv2\n",
       "import numpy as np\n",
       "from ultralytics import YOLO\n",
       "from IPython.display import display, Image\n",
       "import warnings\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Desativando avisos para uma sa√≠da mais limpa\n",
       "warnings.filterwarnings('ignore')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Classifica√ß√£o de Imagens üè∑Ô∏è\n",
       "\n",
       "A classifica√ß√£o de imagens √© a tarefa mais fundamental em vis√£o computacional. O objetivo √© atribuir uma categoria/classe a uma imagem inteira.\n",
       "\n",
       "### O que √© Classifica√ß√£o?\n",
       "- Responde √† pergunta: \"O que est√° nesta imagem?\"\n",
       "- Atribui uma √∫nica categoria √† imagem completa\n",
       "- Base para outras tarefas mais complexas\n",
       "\n",
       "### Aplica√ß√µes Pr√°ticas:\n",
       "- Organiza√ß√£o autom√°tica de fotos\n",
       "- Filtros de conte√∫do\n",
       "- Diagn√≥stico m√©dico por imagem\n",
       "- Controle de qualidade industrial"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def classify(image_path: str):\n",
       "    \"\"\"\n",
       "    Classifica o conte√∫do principal de uma imagem usando YOLOv11.\n",
       "    \n",
       "    Args:\n",
       "        image_path (str): Caminho para a imagem a ser classificada\n",
       "        \n",
       "    Returns:\n",
       "        None: Exibe a imagem com a classifica√ß√£o sobreposta\n",
       "    \"\"\"\n",
       "    # Carregando o modelo pr√©-treinado para classifica√ß√£o\n",
       "    model_cls = YOLO('yolo11n-cls.pt')\n",
       "    \n",
       "    # Fazendo a predi√ß√£o\n",
       "    results = model_cls(image_path, verbose=False)\n",
       "    result = results[0]\n",
       "\n",
       "    # Obtendo a classe com maior confian√ßa\n",
       "    conf = result.probs.top1conf\n",
       "    label = model_cls.names[result.probs.top1]\n",
       "\n",
       "    # Lendo e convertendo a imagem para RGB\n",
       "    img = cv2.imread(image_path)\n",
       "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
       "\n",
       "    # Adicionando texto com a predi√ß√£o\n",
       "    cv2.putText(img, f\"{label}: {conf:.2f}\", (15, 85),\n",
       "                cv2.FONT_HERSHEY_SIMPLEX, 2.6, (139, 0, 0), 5)\n",
       "\n",
       "    # Mostrando a imagem\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    plt.imshow(img)\n",
       "    plt.axis('off')\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Exemplo de classifica√ß√£o\n",
       "classify('/content/alpi.jpg')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Detec√ß√£o de Objetos üéØ\n",
       "\n",
       "A detec√ß√£o de objetos √© uma tarefa mais complexa que combina localiza√ß√£o e classifica√ß√£o. O modelo n√£o apenas identifica o que est√° na imagem, mas tamb√©m onde cada objeto est√° localizado.\n",
       "\n",
       "### O que √© Detec√ß√£o?\n",
       "- Responde √†s perguntas: \"O que s√£o e onde est√£o os objetos?\"\n",
       "- Gera caixas delimitadoras (bounding boxes)\n",
       "- Atribui classes e n√≠veis de confian√ßa\n",
       "\n",
       "### Aplica√ß√µes Pr√°ticas:\n",
       "- Sistemas de vigil√¢ncia\n",
       "- Carros aut√¥nomos\n",
       "- Contagem de objetos\n",
       "- Monitoramento de tr√°fego"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def detect(image_path: str):\n",
       "    \"\"\"\n",
       "    Detecta objetos em uma imagem usando YOLOv11.\n",
       "    \n",
       "    Args:\n",
       "        image_path (str): Caminho para a imagem a ser analisada\n",
       "        \n",
       "    Returns:\n",
       "        None: Exibe a imagem com as detec√ß√µes sobrepostas\n",
       "    \"\"\"\n",
       "    # Carregando o modelo pr√©-treinado para detec√ß√£o\n",
       "    model_det = YOLO('yolo11n.pt')\n",
       "    \n",
       "    # Realizando a detec√ß√£o\n",
       "    results = model_det(image_path, verbose=False)\n",
       "\n",
       "    # Obtendo a imagem com as detec√ß√µes\n",
       "    img_detected = results[0].plot()\n",
       "\n",
       "    # Mostrando a imagem\n",
       "    plt.figure(figsize=(12, 8))\n",
       "    plt.imshow(cv2.cvtColor(img_detected, cv2.COLOR_BGR2RGB))\n",
       "    plt.axis('off')\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Exemplo de detec√ß√£o\n",
       "detect('/content/alpi.jpg')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Segmenta√ß√£o Sem√¢ntica üé®\n",
       "\n",
       "A segmenta√ß√£o sem√¢ntica √© a tarefa mais detalhada em vis√£o computacional. Ela fornece uma compreens√£o pixel a pixel da imagem, identificando precisamente os contornos de cada objeto.\n",
       "\n",
       "### O que √© Segmenta√ß√£o?\n",
       "- Responde √† pergunta: \"Qual √© a forma exata de cada objeto?\"\n",
       "- Cria m√°scaras precisas para cada objeto\n",
       "- Permite an√°lise detalhada da cena\n",
       "\n",
       "### Aplica√ß√µes Pr√°ticas:\n",
       "- Edi√ß√£o autom√°tica de imagens\n",
       "- An√°lise m√©dica e diagn√≥stico\n",
       "- Realidade aumentada\n",
       "- Rob√≥tica de precis√£o"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def segment(image_path: str):\n",
       "    \"\"\"\n",
       "    Realiza segmenta√ß√£o sem√¢ntica em uma imagem usando YOLOv11.\n",
       "    \n",
       "    Args:\n",
       "        image_path (str): Caminho para a imagem a ser segmentada\n",
       "        \n",
       "    Returns:\n",
       "        None: Exibe a imagem com as segmenta√ß√µes sobrepostas\n",
       "    \"\"\"\n",
       "    # Carregando o modelo pr√©-treinado para segmenta√ß√£o\n",
       "    model_seg = YOLO('yolo11n-seg.pt')\n",
       "    \n",
       "    # Realizando a segmenta√ß√£o\n",
       "    results = model_seg(image_path, verbose=False)\n",
       "\n",
       "    # Obtendo a imagem com as segmenta√ß√µes\n",
       "    img_segmented = results[0].plot()\n",
       "\n",
       "    # Mostrando a imagem\n",
       "    plt.figure(figsize=(12, 8))\n",
       "    plt.imshow(cv2.cvtColor(img_segmented, cv2.COLOR_BGR2RGB))\n",
       "    plt.axis('off')\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Exemplo de segmenta√ß√£o\n",
       "segment('/content/alpi.jpg')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. An√°lise em Tempo Real üé•\n",
       "\n",
       "Uma das aplica√ß√µes mais impressionantes da vis√£o computacional √© a an√°lise em tempo real. Vamos demonstrar como realizar detec√ß√£o e segmenta√ß√£o usando a webcam.\n",
       "\n",
       "### Aplica√ß√µes em Tempo Real:\n",
       "- Sistemas de seguran√ßa\n",
       "- Monitoramento de processos\n",
       "- Intera√ß√£o homem-m√°quina\n",
       "- Realidade aumentada"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Fun√ß√µes Auxiliares para Stream de V√≠deo\n",
       "\n",
       "As fun√ß√µes abaixo s√£o necess√°rias para configurar e gerenciar o stream de v√≠deo da webcam no ambiente Jupyter/Colab."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "from IPython.display import display, Javascript, Image\n",
       "from google.colab.output import eval_js\n",
       "from base64 import b64decode, b64encode\n",
       "import cv2\n",
       "import numpy as np\n",
       "import PIL\n",
       "import io\n",
       "import html\n",
       "import time\n",
       "\n",
       "def js_to_image(js_reply):\n",
       "    \"\"\"Converte resposta JavaScript em imagem OpenCV\"\"\"\n",
       "    image_bytes = b64decode(js_reply.split(',')[1])\n",
       "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
       "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
       "    return img\n",
       "\n",
       "def bbox_to_bytes(bbox_array):\n",
       "    \"\"\"Converte array de bbox em string base64\"\"\"\n",
       "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
       "    iobuf = io.BytesIO()\n",
       "    bbox_PIL.save(iobuf, format='png')\n",
       "    bbox_bytes = 'data:image/png;base64,{}'.format(\n",
       "        str(b64encode(iobuf.getvalue()), 'utf-8'))\n",
       "    return bbox_bytes\n",
       "\n",
       "def video_stream():\n",
    "    js = Javascript('''\n",
    "        var video;\n",
    "        var div = null;\n",
    "        var stream;\n",
    "        var captureCanvas;\n",
    "        var imgElement;\n",
    "        var labelElement;\n",
    "\n",
    "        var pendingResolve = null;\n",
    "        var shutdown = false;\n",
    "\n",
    "        function removeDom() {\n",
    "            stream.getVideoTracks()[0].stop();\n",
    "            video.remove();\n",
    "            div.remove();\n",
    "            video = null;\n",
    "            div = null;\n",
    "            stream = null;\n",
    "            imgElement = null;\n",
    "            captureCanvas = null;\n",
    "            labelElement = null;\n",
    "        }\n",
    "\n",
    "        function onAnimationFrame() {\n",
    "            if (!shutdown) {\n",
    "                window.requestAnimationFrame(onAnimationFrame);\n",
    "            }\n",
    "            if (pendingResolve) {\n",
    "                var result = \"\";\n",
    "                if (!shutdown) {\n",
    "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
    "                    result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
    "                }\n",
    "                var lp = pendingResolve;\n",
    "                pendingResolve = null;\n",
    "                lp(result);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async function createDom() {\n",
    "            if (div !== null) {\n",
    "                return stream;\n",
    "            }\n",
    "\n",
    "            div = document.createElement('div');\n",
    "            div.style.border = '2px solid black';\n",
    "            div.style.padding = '3px';\n",
    "            div.style.width = '100%';\n",
    "            div.style.maxWidth = '600px';\n",
    "            document.body.appendChild(div);\n",
    "\n",
    "            const modelOut = document.createElement('div');\n",
    "            modelOut.innerHTML = \"<span>Status:</span>\";\n",
    "            labelElement = document.createElement('span');\n",
    "            labelElement.innerText = 'No data';\n",
    "            labelElement.style.fontWeight = 'bold';\n",
    "            modelOut.appendChild(labelElement);\n",
    "            div.appendChild(modelOut);\n",
    "\n",
    "            video = document.createElement('video');\n",
    "            video.style.display = 'block';\n",
    "            video.width = div.clientWidth - 6;\n",
    "            video.setAttribute('playsinline', '');\n",
    "            video.onclick = () => { shutdown = true; };\n",
    "            stream = await navigator.mediaDevices.getUserMedia(\n",
    "                {video: { facingMode: \"environment\"}});\n",
    "            div.appendChild(video);\n",
    "\n",
    "            imgElement = document.createElement('img');\n",
    "            imgElement.style.position = 'absolute';\n",
    "            imgElement.style.zIndex = 1;\n",
    "            imgElement.onclick = () => { shutdown = true; };\n",
    "            div.appendChild(imgElement);\n",
    "\n",
    "            const instruction = document.createElement('div');\n",
    "            instruction.innerHTML =\n",
    "                '<span style=\"color: red; font-weight: bold;\">' +\n",
    "                'Clique aqui ou no v√≠deo para parar a detec√ß√£o</span>';\n",
    "            div.appendChild(instruction);\n",
    "            instruction.onclick = () => { shutdown = true; };\n",
    "\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "\n",
    "            captureCanvas = document.createElement('canvas');\n",
    "            captureCanvas.width = 640;\n",
    "            captureCanvas.height = 480;\n",
    "            window.requestAnimationFrame(onAnimationFrame);\n",
    "\n",
    "            return stream;\n",
    "        }\n",
    "\n",
    "        async function stream_frame(label, imgData) {\n",
    "            if (shutdown) {\n",
    "                removeDom();\n",
    "                shutdown = false;\n",
    "                return '';\n",
    "            }\n",
    "\n",
    "            var preCreate = Date.now();\n",
    "            stream = await createDom();\n",
    "\n",
    "            var preShow = Date.now();\n",
    "            if (label != \"\") {\n",
    "                labelElement.innerHTML = label;\n",
    "            }\n",
    "\n",
    "            if (imgData != \"\") {\n",
    "                var videoRect = video.getClientRects()[0];\n",
    "                imgElement.style.top = videoRect.top + \"px\";\n",
    "                imgElement.style.left = videoRect.left + \"px\";\n",
    "                imgElement.style.width = videoRect.width + \"px\";\n",
    "                imgElement.style.height = videoRect.height + \"px\";\n",
    "                imgElement.src = imgData;\n",
    "            }\n",
    "\n",
    "            var preCapture = Date.now();\n",
    "            var result = await new Promise(function(resolve, reject) {\n",
    "                pendingResolve = resolve;\n",
    "            });\n",
    "            shutdown = false;\n",
    "\n",
    "            return {'create': preShow - preCreate,\n",
    "                    'show': preCapture - preShow,\n",
    "                    'capture': Date.now() - preCapture,\n",
    "                    'img': result};\n",
    "        }\n",
    "        ''')\n",
    "    display(js)\n",
    "\n",
    "def video_frame(label, bbox):\n",
    "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Detec√ß√£o em Tempo Real üìπ\n",
    "\n",
    "Vamos implementar a detec√ß√£o de objetos em tempo real usando a webcam. Esta demonstra√ß√£o mostra como o YOLOv11 pode processar e analisar v√≠deo em tempo real, identificando e localizando objetos conforme eles aparecem no campo de vis√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_webcam():\n",
    "    \"\"\"Realiza detec√ß√£o de objetos em tempo real usando a webcam.\"\"\"\n",
    "    # Carrega o modelo YOLO\n",
    "    model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "    # Inicia stream de v√≠deo\n",
    "    video_stream()\n",
    "    label_html = 'Detecting...'\n",
    "    bbox = ''\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            js_reply = video_frame(label_html, bbox)\n",
    "            if not js_reply:\n",
    "                break\n",
    "\n",
    "            # Converte resposta JS para imagem OpenCV\n",
    "            img = js_to_image(js_reply[\"img\"])\n",
    "\n",
    "            # Faz a detec√ß√£o com YOLO\n",
    "            results = model(img, verbose=False)\n",
    "\n",
    "            # Cria overlay transparente para bounding boxes\n",
    "            bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
    "\n",
    "            # Desenha as detec√ß√µes\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    conf = float(box.conf[0])\n",
    "                    cls = int(box.cls[0])\n",
    "                    label = f\"{model.names[cls]} {conf:.2f}\"\n",
    "\n",
    "                    # Desenha bbox e label\n",
    "                    bbox_array = cv2.rectangle(bbox_array, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "                    bbox_array = cv2.putText(bbox_array, label, (x1, y1-10),\n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "\n",
    "            # Configura transpar√™ncia\n",
    "            bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
    "            \n",
    "            # Converte para formato adequado\n",
    "            bbox_bytes = bbox_to_bytes(bbox_array)\n",
    "            bbox = bbox_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante a detec√ß√£o: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Iniciar detec√ß√£o em tempo real\n",
    "detect_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Segmenta√ß√£o em Tempo Real üé®\n",
    "\n",
    "Agora vamos demonstrar a segmenta√ß√£o sem√¢ntica em tempo real. Esta √© uma tarefa mais complexa que requer mais poder computacional, mas oferece uma compreens√£o mais detalhada da cena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def segment_webcam():\n",
    "    \"\"\"Realiza segmenta√ß√£o sem√¢ntica em tempo real usando a webcam.\"\"\"\n",
    "    # Carrega modelo de segmenta√ß√£o YOLOv11\n",
    "    model = YOLO(\"yolo11n-seg.pt\")\n",
    "\n",
    "    # Inicia stream de v√≠deo\n",
    "    video_stream()\n",
    "    label_html = 'Segmenting...'\n",
    "    bbox = ''\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            js_reply = video_frame(label_html, bbox)\n",
    "            if not js_reply:\n",
    "                break\n",
    "\n",
    "            # Converte resposta JS para imagem OpenCV\n",
    "            img = js_to_image(js_reply[\"img\"])\n",
    "\n",
    "            # Faz a segmenta√ß√£o com YOLO\n",
    "            results = model(img, verbose=False)\n",
    "\n",
    "            # Pega a imagem com as m√°scaras desenhadas\n",
    "            segmented_frame = results[0].plot()\n",
    "\n",
    "            # Converte para RGBA para poder sobrepor\n",
    "            overlay = cv2.cvtColor(segmented_frame, cv2.COLOR_BGR2RGBA)\n",
    "\n",
    "            # Converte para bytes para mostrar no navegador\n",
    "            bbox_bytes = bbox_to_bytes(overlay)\n",
    "            bbox = bbox_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante a segmenta√ß√£o: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Iniciar segmenta√ß√£o em tempo real\n",
    "segment_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclus√£o üéØ\n",
    "\n",
    "Neste notebook, exploramos as tr√™s principais tarefas da vis√£o computacional moderna:\n",
    "\n",
    "1. **Classifica√ß√£o**: Identifica√ß√£o do conte√∫do principal\n",
    "2. **Detec√ß√£o**: Localiza√ß√£o e identifica√ß√£o de m√∫ltiplos objetos\n",
    "3. **Segmenta√ß√£o**: Delineamento preciso de objetos\n",
    "\n",
    "Tamb√©m demonstramos como estas tarefas podem ser realizadas em tempo real, abrindo possibilidades para diversas aplica√ß√µes pr√°ticas.\n",
    "\n",
    "### Pr√≥ximos Passos üöÄ\n",
    "\n",
    "- Experimente com suas pr√≥prias imagens\n",
    "- Explore diferentes modelos e configura√ß√µes\n",
    "- Considere aplica√ß√µes espec√≠ficas para seu dom√≠nio\n",
    "- Aprenda mais sobre fine-tuning e treinamento personalizado\n",
    "\n",
    "### Recursos Adicionais üìö\n",
    "\n",
    "- [Documenta√ß√£o Ultralytics](https://docs.ultralytics.com/)\n",
    "- [GitHub do Projeto](https://github.com/damarals/scd24-visao-computacional)\n",
    "- [Mais sobre YOLOv11](https://github.com/ultralytics/ultralytics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyNVy2rLh9H5yVQ3LYmW8+f2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}